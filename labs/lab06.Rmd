---
title: "Lab 6"
author: "Adriana Sham"
output: pdf_document
date: "11:59PM March 24, 2019"
---

Load the Boston Housing data and create the vector $y$ and the design matrix $X$.

```{r}
data(Boston, package = "MASS")
y = Boston$medv
intecp = rep(1, nrow(Boston))
X = as.matrix(cbind(intecp, Boston[, 1 : 13]))
```

Find the OLS estimate and OLS predictions without using `lm`.

```{r}
b = solve(t(X) %*% X) %*% t(X) %*% y
b
yhat = X %*% b
yhat
```

Write a function spec'd as follows:

```{r}
#' Orthogonal Projection
#'
#' Projects vector a onto v.
#'
#' @param a   the vector to project
#' @param v   the vector projected onto
#'
#' @returns   a list of two vectors, the orthogonal projection parallel to v named a_parallel, 
#'            and the orthogonal error orthogonal to v called a_perpendicular
orthogonal_projection = function(a, v){
  a_parallel = (v %*% t(v) %*% a) / (sum(v^2))
  a_perpendicular = a - a_parallel
  list("a_parallel" = a_parallel, "a_perpendicular" = a_perpendicular)
}

orthogonal_projection(c(1, 2, 3, 4), c(1, 2, 3, 4))
orthogonal_projection(c(1, 2, 3, 4), c(0, 2, 0, -1)) #parallel is orthogonal
result = orthogonal_projection(c(2, 6, 7, 3), c(1, 3, 5, 7)) #taking 1st c , and projecting it to 2nd c's space
t(result$a_parallel) %*% result$a_perpendicular #equals to zero beause they are orthogonal
result$a_parallel + result$a_perpendicular #getting vector a
#a parallel and v and in the same direction
result$a_parallel / c(1, 3, 5, 7) # 10% shorter 
```


Try to project onto the column space of $X$ by projecting y on each vector of $X$ individually and adding up the projections. You can use the function `orthogonal_projection`.

```{r}
sumOrthProj = rep(0, nrow(X))
for (j in 1 : ncol(X)){
  sumOrthProj = sumOrthProj +orthogonal_projection(y, X[, j])$a_parallel
}
```

How much double counting occurred? Measure the magnitude relative to the true LS orthogonal projection.

```{r}
sumOrthProj / yhat
```

Convert $X$ into $Q$ where $Q$ has the same column space as $X$ but has orthogonal columns. You can use the function `orthogonal_projection`. This is essentially gram-schmidt.

```{r}
# 14 cols, 506
#orthogonal basis
#Q = matrix(NA, nrow(X) , ncol = ncol(X))
#Q[, 1] = X[, 1]
#Q[, 2] = orthogonal_projection(X[, 2], X[, 1])$a_perpendicular
#picture, Q2 is wha is leftover, Q2 and Q1 are orthogonal, span of X1 and X2 making X2 orthogonal with X1
#Q[, 3] = X[, 3] - (orthogonal_projection(X[, 3], Q[, 2]$a_parallel + orthogonal_projection(X[, 3], Q[, 1]$a_parallel)
#Q[, 4] = X[, 4] - (orthogonal_projection(X[, 4], Q[, 3]$a_parallel + orthogonal_projection(X[, 4], Q[, 2]$a_parallel + orthogonal_projection(X[, 4], Q[, 1]$a_parallel)

Q = matrix(NA, nrow = nrow(X), ncol = ncol(X))
Q[, 1] = X[, 1]
for(j in 2 : ncol(X)){
  Q[ , j] = X[ , j]
  
  for(j0 in 1 : (j - 1)){
    Q[ , j] = Q[ , j] - (orthogonal_projection(X[ , j], Q[ , j0])$a_parallel)
  }
}
pacman::p_load(Matrix)
rankMatrix(Q)
dim(Q)
ncol(X)

t(Q) %*% Q
```

Make $Q$'s columns orthonormal.

```{r}
#each column are already orthogonal, so now find normal of each column
for (j in 1:ncol(Q)){
  Q[, j] =  Q[, j] / sqrt(sum(Q[, j]^2))
}
head(Q)
```

Verify $Q^T$ is the inverse of $Q$.

```{r}
t(Q) %*% Q
```


Project $Y$ onto $Q$ and verify it is the same as the OLS fit.

```{r}
cbind( Q %*% t(Q) %*% y, yhat)
```


Project $Y$ onto the columns of $Q$ one by one and verify it sums to be the projection onto the whole space.

```{r}
yq_projection = Q %*% diag(ncol(Q)) %*% t(Q) %*% y
yq_columns_projection = matrix(nrow = nrow(Q), ncol = 0)
for(j in 1:ncol(Q)){
  yq_columns_projection = cbind(yq_columns_projection, Q[, j] %*% t(Q[, j]) %*% y)
}
#sum_of_space = cbind(yq_projection, as.matrix(rowSums(yq_columns_projection))
#sum_of_space
```

Verify the OLS fit squared lengh is the sum of squared lengths of each of the orthogonal projections.

```{r}
sum(t(yq_columns_projection) %*% yq_columns_projection)
t(yhat) %*% yhat
```

Rewrite the "The monotonicity of SSR" demo from the lec06 notes. Comment every line in detail. Write about what the plots means.

```{r}
n = 100
y = rnorm(n)
RMSE = array(NA, n)

#Residual of the null (i.e. just regressing on the intercept)
RMSE[1] = 1 

#create a matrix with the correct number of rows but no columns
X = matrix(NA, nrow = n, ncol = 0)
X = cbind(1, X) #intercept

#for every new p, tack on a new random continuos predictor:
for (p_plus_one in 2 : n){
  X = cbind(X, rnorm(n))   #tack new RSME onto RSME list
  RMSE[p_plus_one] = summary(lm(y ~ X))$sigma
}

pacman::p_load(ggplot2)
base = ggplot(data.frame(p_plus_one = 1 : n, RMSE = RMSE))
base + geom_line(aes(x = p_plus_one, y = RMSE))

```

Rewrite the "Overfitting" demo from the lec06 notes. Comment every line in detail. Write about what the plots means.

```{r}
bbeta = c(1, 2, 3, 4) #the weights vector

#build training data
n = 100 #rows
X = cbind(1, rnorm(n), rnorm(n), rnorm(n)) #p + 1
y = X %*% bbeta + rnorm(n, 0, 0.3)

#build test data
n_star = 100 #new number of rows
X_star = cbind(1, rnorm(n), rnorm(n), rnorm(n_star)) #out of sample p + 1
y_star = X_star %*% bbeta + rnorm(n, 0, 0.3)

all_betas = matrix(NA, n, n) #creates an n by n matrix fill with NAs
all_betas[4, 1 : 4] = coef(lm(y ~ 0 + X))
in_sample_rmse_by_p = array(NA, n)
for (j in 5 : n){
  X = cbind(X, rnorm(n))
  lm_mod = lm(y ~ 0 + X)
  all_betas[j, 1 : j] = coef(lm_mod)
  y_hat = X %*% all_betas[j, 1 : j]
  in_sample_rmse_by_p[j] = sqrt(sum((y - y_hat)^2))
}
plot(1 : n, in_sample_rmse_by_p)

all_betas[4 : n, 1 : 4]
b_error_by_p = rowSums((all_betas[, 1 : 4] - matrix(rep(bbeta, n), nrow = n, byrow = TRUE))^2)
plot(1 : n, b_error_by_p)

#look at out of sample error in the case of only the first four features
oos_rmse_by_p = array(NA, n)
for (j in 4 : n){
  y_hat_star = X_star %*% all_betas[j, 1 : 4]
  oos_rmse_by_p[j] = sqrt(sum((y_star - y_hat_star)^2))
}
plot(1 : n, oos_rmse_by_p)

#look at out of sample error in the case of the random features too
oos_rmse_by_p = array(NA, n)
for (j in 5 : n){
  X_star = cbind(X_star, rnorm(n))
  y_hat_star = X_star %*% all_betas[j, 1 : j]
  oos_rmse_by_p[j] = sqrt(sum((y_star - y_hat_star)^2))
}
plot(1 : n, oos_rmse_by_p)
```
